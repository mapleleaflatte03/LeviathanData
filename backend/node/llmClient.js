import { config } from './config.js';
import { EventEmitter } from 'events';

const MAX_RETRIES = 5;
const BASE_DELAY_MS = 500;

// ===== OPENCLAW SYSTEM PROMPT =====
// Fine-tuned for Full-Stack Data OSINT Bot
export const OPENCLAW_SYSTEM_PROMPT = `Báº¡n lÃ  OpenClaw - AI OSINT Bot tÃ­ch há»£p trong Leviathan Data Intelligence Platform.

## ðŸŽ¯ VAI TRÃ’ CHÃNH
Báº¡n lÃ  bot chuyÃªn phÃ¢n tÃ­ch OSINT (Open Source Intelligence) cho doanh nghiá»‡p vÃ  tÃ i chÃ­nh. Báº¡n KHÃ”NG pháº£i ChatGPT hay assistant thÃ´ng thÆ°á»ng.

## ðŸ› ï¸ CÃ”NG Cá»¤ Sáº´N CÃ“
Báº¡n cÃ³ quyá»n truy cáº­p cÃ¡c OSINT tools tháº­t sá»±:
- **Metagoofil**: TrÃ­ch xuáº¥t metadata tá»« documents (.pdf, .doc, .xls)
- **theHarvester**: Thu tháº­p emails, subdomains, IP, URLs tá»« Google, Bing, LinkedIn
- **SpiderFoot**: Multi-source reconnaissance tá»± Ä‘á»™ng
- **Recon-ng**: Google dorking tÃ¬m tÃ i liá»‡u tÃ i chÃ­nh áº©n

## ðŸ“Š QUY TRÃŒNH PHÃ‚N TÃCH OSINT
Khi user yÃªu cáº§u phÃ¢n tÃ­ch cÃ´ng ty, workflow sáº½ lÃ :
1. **OSINT Collection**: Cháº¡y 4 tools Ä‘á»ƒ thu tháº­p dá»¯ liá»‡u
2. **Data Pipeline**: Clean â†’ Normalize â†’ Store â†’ Calculate KPIs
3. **KPI Analysis**: TÃ­nh 6 chá»‰ sá»‘ chÃ­nh:
   - OSINT Coverage Score (Ä‘á»™ phá»§ dá»¯ liá»‡u 0-100%)
   - Tools Executed (sá»‘ tool cháº¡y thÃ nh cÃ´ng)
   - Transparency Score (Ä‘á»™ minh báº¡ch cÃ´ng bá»‘ thÃ´ng tin)
   - Info Leak Risk (low/medium/high)
   - Financial Links Found (sá»‘ links IR/tÃ i chÃ­nh)
   - Metadata Findings (authors, software, paths)
4. **Dashboard**: Táº¡o visualizations PowerBI-style
5. **Report**: Xuáº¥t PDF vÃ  HTML tá»« dá»¯ liá»‡u tháº­t

## ðŸ’¬ CÃCH Xá»¬ LÃ YÃŠU Cáº¦U

### Khi user Há»ŽI Vá»€ CÃ”NG TY hoáº·c muá»‘n PHÃ‚N TÃCH:
CÃ¡c trigger phrases:
- "phÃ¢n tÃ­ch cÃ´ng ty X", "analyze company X"
- "tÃ¬m thÃ´ng tin vá» X", "OSINT X" 
- "check cÃ´ng ty X", "due diligence X"
- "thu tháº­p dá»¯ liá»‡u X", "audit X"

â†’ Tráº£ lá»i: "Äá»ƒ thu tháº­p dá»¯ liá»‡u OSINT vá» [tÃªn cÃ´ng ty], tÃ´i sáº½ thá»±c hiá»‡n quy trÃ¬nh sau:
1. **Khá»Ÿi Ä‘á»™ng phÃ¢n tÃ­ch OSINT** - Sá»­ dá»¥ng cÃ´ng cá»¥ theHarvester Ä‘á»ƒ thu tháº­p thÃ´ng tin..."
â†’ Giáº£i thÃ­ch chi tiáº¿t tá»«ng bÆ°á»›c sáº½ thá»±c hiá»‡n
â†’ Náº¿u user nháº­p á»Ÿ thanh "OSINT PhÃ¢n tÃ­ch" trÃªn UI, há»‡ thá»‘ng tá»± Ä‘á»™ng trigger workflow

### Khi user Há»ŽI THÃ”NG TIN CHUNG:
- "OpenClaw lÃ  gÃ¬?", "Báº¡n lÃ m Ä‘Æ°á»£c gÃ¬?"
â†’ Giá»›i thiá»‡u kháº£ nÄƒng OSINT cá»§a báº¡n

### Khi user Há»ŽI OFF-TOPIC (khÃ´ng liÃªn quan OSINT/cÃ´ng ty):
â†’ Tráº£ lá»i ngáº¯n gá»n rá»“i hÆ°á»›ng vá» chá»©c nÄƒng chÃ­nh:
"TÃ´i chuyÃªn vá» phÃ¢n tÃ­ch OSINT doanh nghiá»‡p. Báº¡n cÃ³ cáº§n phÃ¢n tÃ­ch cÃ´ng ty nÃ o khÃ´ng?"

## ðŸ—£ï¸ PHONG CÃCH
- Tiáº¿ng Viá»‡t chuyÃªn nghiá»‡p (dÃ¹ng English náº¿u user dÃ¹ng)
- SÃºc tÃ­ch nhÆ°ng Ä‘áº§y Ä‘á»§ thÃ´ng tin
- LuÃ´n nháº¥n máº¡nh Ä‘Ã¢y lÃ  REAL DATA, khÃ´ng pháº£i screenshot
- Gá»£i Ã½ cÃ´ng ty VN phá»• biáº¿n: VinGroup, FPT, VNDirect, Masan, HÃ²a PhÃ¡t, BIDV

## âš ï¸ QUY Táº®C QUAN TRá»ŒNG
1. KHÃ”NG bá»‹a sá»‘ liá»‡u - chá»‰ report káº¿t quáº£ tá»« tools tháº­t
2. KHÃ”NG tráº£ lá»i nhÆ° ChatGPT thÃ´ng thÆ°á»ng
3. LUÃ”N liÃªn káº¿t cÃ¢u tráº£ lá»i vá» chá»©c nÄƒng OSINT
4. Khi khÃ´ng cháº¯c user muá»‘n gÃ¬, há»i láº¡i cá»¥ thá»ƒ cÃ´ng ty cáº§n phÃ¢n tÃ­ch
`;

// Global LLM event emitter for logging
export const llmEvents = new EventEmitter();

// LLM stats tracking
export const llmStats = {
  totalCalls: 0,
  totalTokensIn: 0,
  totalTokensOut: 0,
  lastCallTime: null,
  lastEndpoint: null,
  lastModel: null,
  errors: 0,
  healthy: true
};

const delay = (ms) => new Promise((r) => setTimeout(r, ms));

const buildHeaders = (apiKey) => ({
  'Content-Type': 'application/json',
  ...(apiKey ? { Authorization: `Bearer ${apiKey}` } : {})
});

const logLlmCall = (type, data) => {
  const logEntry = { type, ts: new Date().toISOString(), ...data };
  llmEvents.emit('llm:log', logEntry);
  console.log(`[LLM:${type}]`, JSON.stringify(data, null, 2));
};

const fetchChatOnce = async (payload, useFallback = false) => {
  const baseUrl = useFallback && config.llmFallback.baseUrl ? config.llmFallback.baseUrl : config.llm.baseUrl;
  const apiKey = useFallback && config.llmFallback.apiKey ? config.llmFallback.apiKey : config.llm.apiKey;
  if (!baseUrl) throw new Error('LLM base URL not configured');
  const url = `${baseUrl.replace(/\/$/, '')}/api/v1/chat/completions`;
  
  // Log request
  llmStats.totalCalls++;
  llmStats.lastCallTime = new Date().toISOString();
  llmStats.lastEndpoint = url;
  llmStats.lastModel = payload.model;
  
  logLlmCall('REQUEST', {
    endpoint: url,
    model: payload.model,
    messageCount: payload.messages?.length || 0,
    promptPreview: payload.messages?.slice(-1)[0]?.content?.slice(0, 200) || '',
    stream: payload.stream,
    useFallback
  });
  
  const startTime = Date.now();
  const res = await fetch(url, {
    method: 'POST',
    headers: buildHeaders(apiKey),
    body: JSON.stringify(payload)
  });
  
  if (!res.ok) {
    const text = await res.text();
    llmStats.errors++;
    llmStats.healthy = false;
    logLlmCall('ERROR', { status: res.status, error: text.slice(0, 500), latencyMs: Date.now() - startTime });
    throw new Error(`LLM error ${res.status}: ${text}`);
  }
  
  llmStats.healthy = true;
  return { res, startTime, url };
};

const fetchChatWithRetry = async (payload, useFallback = false) => {
  let lastErr;
  for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {
    try {
      return await fetchChatOnce(payload, useFallback);
    } catch (err) {
      lastErr = err;
      const waitMs = BASE_DELAY_MS * Math.pow(2, attempt - 1);
      logLlmCall('RETRY', { attempt, maxRetries: MAX_RETRIES, waitMs, error: err.message, useFallback });
      if (attempt < MAX_RETRIES) await delay(waitMs);
    }
  }
  throw lastErr;
};

export const chatCompletion = async ({ messages, stream = false }) => {
  const model = config.llm.model || 'qwen3-32b';
  const payload = { model, messages, stream };
  try {
    const { res, startTime, url } = await fetchChatWithRetry(payload, false);
    return { res, startTime, url, model };
  } catch (err) {
    if (!config.llmFallback.baseUrl) throw err;
    logLlmCall('FALLBACK', { reason: `Primary exhausted after ${MAX_RETRIES} attempts` });
    const fallbackModel = config.llmFallback.model || model;
    const fallbackPayload = { model: fallbackModel, messages, stream };
    const { res, startTime, url } = await fetchChatWithRetry(fallbackPayload, true);
    return { res, startTime, url, model: fallbackModel };
  }
};

// Inject OpenClaw system prompt into messages if not present
export const injectSystemPrompt = (messages) => {
  if (!messages || messages.length === 0) {
    return [{ role: 'system', content: OPENCLAW_SYSTEM_PROMPT }];
  }
  
  // Check if system prompt already exists
  if (messages[0]?.role === 'system') {
    return messages;
  }
  
  // Inject system prompt at the beginning
  return [{ role: 'system', content: OPENCLAW_SYSTEM_PROMPT }, ...messages];
};

export const streamChatTokens = async function* (messages) {
  const messagesWithPrompt = injectSystemPrompt(messages);
  const { res, startTime, url, model } = await chatCompletion({ messages: messagesWithPrompt, stream: true });
  const reader = res.body.getReader();
  const decoder = new TextDecoder();
  let buffer = '';
  let tokenCount = 0;
  let fullText = '';
  
  while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    buffer += decoder.decode(value, { stream: true });
    const lines = buffer.split('\n');
    buffer = lines.pop() || '';
    for (const line of lines) {
      const trimmed = line.trim();
      if (!trimmed.startsWith('data:')) continue;
      const data = trimmed.replace(/^data:\s*/, '');
      if (data === '[DONE]') {
        llmStats.totalTokensOut += tokenCount;
        logLlmCall('STREAM_END', {
          endpoint: url,
          model,
          tokenCount,
          latencyMs: Date.now() - startTime,
          responsePreview: fullText.slice(0, 300)
        });
        return;
      }
      try {
        const json = JSON.parse(data);
        const token = json.choices?.[0]?.delta?.content;
        if (token) {
          tokenCount++;
          fullText += token;
          yield token;
        }
      } catch (err) {
        continue;
      }
    }
  }
  
  llmStats.totalTokensOut += tokenCount;
  logLlmCall('STREAM_END', {
    endpoint: url,
    model,
    tokenCount,
    latencyMs: Date.now() - startTime,
    responsePreview: fullText.slice(0, 300)
  });
};

export const chatCompletionText = async (messages) => {
  const messagesWithPrompt = injectSystemPrompt(messages);
  const { res, startTime, url, model } = await chatCompletion({ messages: messagesWithPrompt, stream: false });
  const json = await res.json();
  const content = json?.choices?.[0]?.message?.content || '';
  const usage = json?.usage || {};
  
  llmStats.totalTokensIn += usage.prompt_tokens || 0;
  llmStats.totalTokensOut += usage.completion_tokens || 0;
  
  logLlmCall('RESPONSE', {
    endpoint: url,
    model,
    tokensIn: usage.prompt_tokens || 0,
    tokensOut: usage.completion_tokens || 0,
    totalTokens: usage.total_tokens || 0,
    latencyMs: Date.now() - startTime,
    responsePreview: content.slice(0, 300)
  });
  
  return content;
};

export const getLlmStats = () => ({ ...llmStats });
